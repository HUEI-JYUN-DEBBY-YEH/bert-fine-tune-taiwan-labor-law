import pandas as pd
import json
from datasets import Dataset
from transformers import AutoTokenizer

def build_label2id(labels):
    unique_labels = sorted(set(labels))
    return {label: idx for idx, label in enumerate(unique_labels)}

def main():
    # è¼¸å…¥æª”æ¡ˆè·¯å¾‘
    csv_path = "laborlaw_sentences_labeled.csv"  # æ¨™è¨»å®Œæˆçš„ CSV
    model_name = "bert-base-chinese"             # å¯æ›¿æ›ç‚ºå…¶ä»–æ¨¡å‹åç¨±

    # è®€å– CSV
    df = pd.read_csv(csv_path)
    df = df.dropna(subset=["text", "label"])     # ç§»é™¤ç¼º label è³‡æ–™

    # å»ºç«‹ label2id å°æ‡‰è¡¨
    label2id = build_label2id(df["label"])
    print("âœ… label2id æ˜ å°„ï¼š", label2id)

    # å°‡æ–‡å­— label è½‰ç‚ºæ•¸å€¼
    df["label_id"] = df["label"].map(label2id)

    # å»ºç«‹ Hugging Face Dataset
    dataset = Dataset.from_pandas(df[["text", "label_id"]])

    # è¼‰å…¥ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # å®šç¾© tokenizer å‡½å¼
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=128
        )

    def preprocess(example):
        encoding = tokenizer(
            example["text"],
            truncation=True,
            padding="max_length",
            max_length=128
        )
        encoding["labels"] = example["label"]  # âœ… åŠ é€™ä¸€è¡Œ
        return encoding

    # å° dataset åš tokenizer è™•ç†
    def tokenize_and_add_labels(example):
        encoding = tokenizer(
            example["text"],
            padding="max_length",
            truncation=True,
            max_length=128
        )
        encoding["labels"] = example["label_id"]  # ğŸ”¥ åŠ å…¥ labels æ¬„ä½
        return encoding

    tokenized_dataset = dataset.map(tokenize_and_add_labels)

    print("âœ… Tokenizer è™•ç†å®Œæˆ")
    print(tokenized_dataset.column_names)
    # æ‡‰è©²åŒ…å«ï¼š['input_ids', 'token_type_ids', 'attention_mask', 'labels']
    print(tokenized_dataset[0])

    # åˆ†å‰² train/testï¼ˆ80/20ï¼‰
    split_dataset = tokenized_dataset.train_test_split(test_size=0.2)
    split_dataset.save_to_disk("tokenized_dataset")

    # å„²å­˜ label2id å°æ‡‰è¡¨
    with open("label2id.json", "w", encoding="utf-8") as f:
        json.dump(label2id, f, ensure_ascii=False, indent=2)
    print("âœ… Dataset èˆ‡ label2id å·²å„²å­˜å®Œç•¢")

if __name__ == "__main__":
    main()
