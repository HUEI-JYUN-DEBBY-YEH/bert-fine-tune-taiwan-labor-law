import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import json
from plot_metrics import plot_classification_report
from PIL import Image

# === æ¨¡å‹èˆ‡ tokenizer è·¯å¾‘ ===
MODEL_PATH = "finetuned_laborlaw_model"
LABEL_MAPPING_PATH = './label2id.json'
REPORT_PATH = './classification_report.json'  # é å…ˆå„²å­˜çš„ dict

# === è¼‰å…¥ tokenizer & æ¨¡å‹ ===
@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
    model.eval()
    return tokenizer, model

@st.cache_data
def load_labels():
    with open(LABEL_MAPPING_PATH, 'r', encoding='utf-8') as f:
        label2id = json.load(f)
    id2label = {v: k for k, v in label2id.items()}
    return id2label

# === é æ¸¬é‚è¼¯ ===
def predict(text, tokenizer, model, id2label):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.softmax(outputs.logits, dim=1).squeeze()
    top_id = torch.argmax(probs).item()
    return id2label[top_id], probs[top_id].item()

# === ä»‹é¢é–‹å§‹ ===
st.title("ğŸ§  å‹åŸºæ³•æ–‡æœ¬åˆ†é¡ Demo")
st.markdown("æœ¬æ¨¡å‹ç‚º fine-tuned BERTï¼Œæ”¯æ´å¤šé¡åˆ¥æ–‡æœ¬åˆ†é¡")

tokenizer, model = load_model()
id2label = load_labels()

text_input = st.text_area("è«‹è¼¸å…¥å‹åŸºæ³•ç›¸é—œæ–‡æœ¬ï¼š", height=150)

if st.button("é æ¸¬é¡åˆ¥"):
    if text_input.strip():
        label, score = predict(text_input, tokenizer, model, id2label)
        st.success(f"**é æ¸¬é¡åˆ¥ï¼š{label}**ï¼ˆä¿¡å¿ƒåº¦ï¼š{score:.2f}ï¼‰")
    else:
        st.warning("è«‹å…ˆè¼¸å…¥æ–‡æœ¬")

# === é¡¯ç¤ºè©•ä¼°åœ–è¡¨ ===
with st.expander("ğŸ“Š æŸ¥çœ‹æ¨¡å‹åˆ†é¡è¡¨ç¾"):
    with open(REPORT_PATH, 'r', encoding='utf-8') as f:
        report = json.load(f)
    plot_classification_report(report)
    st.image(Image.open("metrics_plot.png"), caption="Classification Report")

st.markdown("---")
st.caption("Â© 2025 å‹åŸºæ³• NLP åˆ†é¡å°ˆæ¡ˆ | Powered by BERT")